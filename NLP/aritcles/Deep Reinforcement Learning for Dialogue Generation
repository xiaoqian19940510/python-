题目：Deep Reinforcement Learning for Dialogue Generation
使用的相关技术：强化学习，深度增强学习，LSTM（简要说明），seq2seq对话模型（简要说明）
1、	论述问什么使用强化学习
回报最大化，不会直接指示选择哪种行动，一系列的actions和奖励信号都会对以后有影响，而其他机器学习算法是学习怎么做。选择强化学习的原因：（1）相比监督学习，监督学习的label是其应该执行的正确行为，而对交互的问题处理效果不好，不能从自己的经历中学习，不能对当前的信息评估以优化下一个行为。（2）无监督学习是从一堆样本中发现其隐藏的结构，不适用。所以少RL不需要label，只需要reward信号，反馈是延迟的，不是立即生成的，agent的行为会影响之后的data，刚好适用于对话系统中。
2、	介绍强化学习（策略函数和价值函数）马尔科夫的过程
不是告诉agent如何产生正确的动作，而是agent对所产生的动作评估其好坏，agent根据自己的学习经历训练出更符合自身特点的网络，通过agent对动作的评估来学习，以适应环境。其目标是动态地调整参数，以达到强化信号最大。所以系统中需要一个随机单元，使agent在可能的动作空间中进行搜索并发现正确的动作，具有动态学习的特性。强化学习中两个重要的概念是：探索（exploration新的东西）和开发（exploitation），不去探索可能找到的是局部最优值。RL问题四个重要概念：（1）规则（policy）：agents在特定时间特定环境的行为方式；（2）奖励信号（a reward signal）：每个时间点agent返回给另一个agent的信号，根据reward来调整policy；（3）值函数（value function）：相比reward是长期收益，可以看作是reward的累加；（4）环境模型（a model of the environment）：agent和environment交互见下图。。。。总结即寻找策略使回报最大。
3、	介绍深度增强学习
Q学习：在状态s下执行动作a，当游戏结束时，可能获得的最大分数，表示的是下一个动作的质量。通过Bellman方程来迭代求解Q函数。深度Q网络，通过神经网络的前向计算所有动作的Q值，选择Q值最高的动作，并进行相应的Q值更新，更新是使用神经网络的反向传播。但可能存在局部最优值，加入随机因子。
（1）	用一个深度神经网络作为Q值得网络，参数为权重；
（2）	损失函数：Q值中的均方差；Q的当前值和Q的目标值来计算均方差和偏差；
（3）	损失函数的梯度，计算参数；
（4）	使用SGD实现End-to-end的优化目标，从而得到最优的Q值。
4、	论述本文如何将深度增强学习应用与对话系统
本文通过与4层LSTM编解码训练模型和利用强化学习训练的网络作比较，结果强化学习的方法达到更好的效果。
 

研究问题：已有的SEQ2SEQ模型虽然能够生成对话，但对话持久性差且对话内容容易形成环状，考虑信息性，连贯性和回答的简易性，设计深度强化学习网络，使回答的内容更具前瞻性，使对话时间更持久，且给予用户更综合性的回复。
使用方法：学习alphago的设计思路，设计具有两个代理的强化学习网络，一个代理使用的是原来生成的seq2seq对话模型，另一个代理使用，。为使对话更具多样性，使用最大熵模型。通过两个代理之间的学习，提高网络性能。初始化使用最大似然估计的参数编码器解码器RNN，策略网络使用LSTM编解码，以对话内容的多样性和对话轮数为评估标准。
最终效果：实验结果表明给予强化学习的模型确实会产生更多的交互，更倾向于让用户更多的倾诉，模型提高对话内容的多样性和对话轮数。如下图：
 
5、	搜索是否有相关代码




