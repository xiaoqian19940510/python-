这篇文章主要的贡献就是第一次将GAN应用到了NLP的传统任务上面，而且BLEU有2的提升。

这个模型他们称之为CSGAN-NMT，G用的是传统的attention-basedNMT模型，而D有两种方案，一种是CNNbased，另一种是RNNbased，通过实验比较发现CNN的效果更好。推测的原因是RNN的分类模型在训练早期能够有极高的分类准确率，导致总能识别出G生成的数据和真实的数据，G难以训练（因为总是negativesignal）,

这篇文章的重点我想是4.训练策略，GAN极难训练，他们首先是用MLE来pretrainG，然后再用G生成的样本和真实样本来pretrainD，当D达到某一个准确率的时候，进入对抗性训练的环节，GAN的部分基本和SeqGAN一样，用policygradientmethod+MCsearch，上面已经讲过了不再重复。但是由于在对抗性训练的时候，G没有直接接触到goldentargetsentence，所以每用policygradient更新一次G都跑一次professorforcing。这里我比较困惑，我觉得是不是像Jiwei那篇文章，是用D给出的Reward来更新G参数之后，又用MLE来更新一次G的参数（保证G能接触到真实的样本，这里就是目标语言的序列），但这个方法是teacher-forcing不是professorforcing。

最后就是训练Trick茫茫，这篇文章试了很多超参数，比如D要pretrain到f=0.82的时候效果最好，还有pretrain要用Adam，而对抗性训练要用RMSProp，同时还要和WGAN一样将每次更新D的权重固定在一个范围之内。
