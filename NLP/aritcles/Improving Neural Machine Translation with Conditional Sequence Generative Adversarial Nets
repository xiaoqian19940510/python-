这篇文章主要的贡献就是第一次将GAN应用到了NLP的传统任务上面，而且BLEU有2的提升。

这个模型他们称之为CSGAN-NMT，G用的是传统的attention-basedNMT模型，而D有两种方案，一种是CNNbased，另一种是RNNbased，通过实验比较发现CNN的效果更好。推测的原因是RNN的分类模型在训练早期能够有极高的分类准确率，导致总能识别出G生成的数据和真实的数据，G难以训练（因为总是negativesignal）,

这篇文章的重点我想是4.训练策略，GAN极难训练，他们首先是用MLE来pretrainG，然后再用G生成的样本和真实样本来pretrainD，当D达到某一个准确率的时候，进入对抗性训练的环节，GAN的部分基本和SeqGAN一样，用policygradientmethod+MCsearch，上面已经讲过了不再重复。但是由于在对抗性训练的时候，G没有直接接触到goldentargetsentence，所以每用policygradient更新一次G都跑一次professorforcing。这里我比较困惑，我觉得是不是像Jiwei那篇文章，是用D给出的Reward来更新G参数之后，又用MLE来更新一次G的参数（保证G能接触到真实的样本，这里就是目标语言的序列），但这个方法是teacher-forcing不是professorforcing。

最后就是训练Trick茫茫，这篇文章试了很多超参数，比如D要pretrain到f=0.82的时候效果最好，还有pretrain要用Adam，而对抗性训练要用RMSProp，同时还要和WGAN一样将每次更新D的权重固定在一个范围之内。


　导读

　　这篇文章的主要工作在于应用了对抗训练（adversarial training）的思路来解决神经机器翻译（Neural Machine Translation）的问题。在此之前实际上在NLP的任务上，已经有不少关于GAN的尝试，不过任务相对受限，效果也不是十分理想，调试的细节也没有很详实的说明。这篇工作在模型上与最近其他在NLP上GAN的尝试很类似，仍旧就是将整体任务划分到两个子系统上，一个是生成器（generative model），利用seq2seq式的模型以源语言的句子作为输入，输出对应的目标语言的翻译；另一个则是一个判别器（discriminator），用以区分源语言和目标语言是否是翻译正确的，判别手段同样是将目标语言和源语言的句子向量化之后输入到一个二分类器中（或者一个0~1的概率判别器）。我们在之前的Jiwei Li的Adversarial Learning for Neural Dialogue Generation中已经介绍过这样的训练方式的意义所在，其主要的意义还是用来解决人为量化标准导致的训练偏差，用一种接近图论测试的方式来介入训练。虽然模型上并没有过多的创新，但本篇工作对于GAN的实验细节有不少交待并且从实验结果上看也有了不少的提升，并且更为重要的是在一个十分传统的NLP领域上使用了GAN，在一定程度上进一步验证了使用增强学习实现GAN反馈的可行性。

　　2

　　模型

　　Generativemodel & Discriminative model

　　本文的生成器G采用了一个传统的基于attention机制的NMT模型（https://github.com/nyu-dl/dl4mt-tutorial），在此不作过多赘述。而在判别器D部分，本文采用了CNN和RNN两套不同的网络系统来进行，将源语言和目标语言的句子分别向量化之后通过一个logistic函数来得到一致概率。



　　PolicyGradient Training

　　和之前一样，本文同样采用了增强学习来进行D到G的反馈，同样是采用policygradient来进行增强学习的训练，其优化目标为：

　　

　　在模型中，state为源语言以及已生成的部分目标语言语句，决策为目标语句的下一个生成词。其中R是针对生成结果的反馈函数，而G则是生成的概率分布。policy gradient整体的优化目标其实就是希望回报高的决策其概率越高，换言之就是翻译越准的决策在概率分布上的概率越大。

　　R的定义也非常明确，如果是最终状态的话那么直接可以得到源语言和目标语言语句的接近程度。而对于中间状态的评估和之前一样，采取了Monte Carlo，通过随机生成的最终语句评价均值来衡量，具体展开如下：

　　

　　

　　而整个增强学习的求导如下：

　　

　　Professor Forcing

　　在以往的工作中，D效果非常好而G的效果非常糟糕会带来训练效果的下降。试想一下一个G所有产生的答案都被D驳回了，在这段时间内G的所有反馈都是负反馈，G就会迷失从而不知道向什么方向优化会得到正反馈，所以理想的情况下G和D是交替训练上升的。在控制D和G训练节奏的同时，这篇工作中同样采用了强制学习来解决问题，这个在之前的很多工作中都有过介绍了。

　　Training strategies

　　G的预处理需要将模型在之前的训练数据上达到一个最好效果时再加入到GAN中进行进一步训练而不是直接就是一个几乎没有性能的生成器。在此之后利用G生成一批数据和标注数据一起来给D进行预训练。所有的预训练使用Adam，而在之后的GAN的训练则使用RMSProp 来进行梯度反馈。更多的细节可以详细阅读论文获得。

　　3

　　实验

　　从汇报的实验结果来看，效果还是比较明显的，和基本的baseline相比有不小的提升。

　　

　　4

　　总结

　　这篇工作的最大亮点在于对比较传统的机器翻译任务进行了GAN的尝试，并且取得了不错的效果，尽管在模型上和以往在NLP上使用的GAN相比没有太多的变化。而针对实验的调试细节和参数也比较详细，和之前公布的一些开源代码结合，可以对之后在NLP中使用GAN提供了便利。而且和过去的论文结合起来看，使用增强学习，尤其是Policy Gradient Training+ Professor Forcing的GAN实现套路得到了相当的验证，未来其他的一些生成任务也可以在这个框架上进行尝试。
