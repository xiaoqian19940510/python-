在Reddit上就点评了这篇文章，NYU的又祭出了这篇，令WGAN在NLP上也能发挥威力。

在WGAN中，他们给出的改进方案是：

判别器最后一层去掉sigmoid

生成器和判别器的loss不取log

每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c

不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行

这里引用自知乎专栏：令人拍案叫绝的WassersteinGAN-知乎专栏

文章写得深入浅出，强烈推荐。

其中第三项就是机器翻译文章中也用到的weightclipping，在本文中，他们发现通过weightclipping来对D实施Lipschitz限制（为了逼近难以直接计算的Wasserstein距离），是导致训练不稳定，以及难以捕捉复杂概率分布的元凶。所以文章提出通过梯度惩罚来对Critic（也就是D，WGAN系列都将D称之为Critic）试试Lipschitz限制。

如图：损失函数有原来的部分+梯度惩罚，现在不需要weightclipping以及基于动量的优化算法都可以使用了，他们在这里就用了Adam。同时可以拿掉BatchNormalization。

如图所示，实验结果很惊人，这种WGAN—GP的结构，训练更加稳定，收敛更快，同时能够生成更高质量的样本，而且可以用于训练不同的GAN架构，甚至是101层的深度残差网络。

同时也能用于NLP中的生成任务，而且是character-level的languagemodel，而MaliGAN的实验是在Sentence-Level上面的。而且前面几篇提到的文章2,3,4在对抗性训练的时候或多或少都用到了MLE，令G更够接触到GroundTruth，但是WGAN-GP是完全不需要MLE的部分。


生成对抗网络（GAN）是一种强大的生成模型，但是自从2014年Ian Goodfellow提出以来，GAN就存在训练不稳定的问题。最近提出的 Wasserstein GAN（WGAN）在训练稳定性上有极大的进步，但是在某些设定下仍存在生成低质量的样本，或者不能收敛等问题。

近日，蒙特利尔大学的研究者们在WGAN的训练上又有了新的进展，他们将论文《Improved Training of Wasserstein GANs》发布在了arXiv上。研究者们发现失败的案例通常是由在WGAN中使用权重剪枝来对critic实施Lipschitz约束导致的。在本片论文中，研究者们提出了一种替代权重剪枝实施Lipschitz约束的方法：惩罚critic对输入的梯度。该方法收敛速度更快，并能够生成比权重剪枝的WGAN更高质量的样本。

WGAN 归纳
在 WGAN 中，D 的任务不再是尽力区分生成样本与真实样本，而是尽量拟合出样本间的 Wasserstein 距离，从分类任务转化成回归任务。而 G 的任务则变成了尽力缩短样本间的 Wasserstein 距离。

故 WGAN 对原始 GAN 做出了如下改变:

D 的最后一层取消 sigmoid
D 的 w 取值限制在 [-c,c] 区间内。
使用 RMSProp 或 SGD 并以较低的学习率进行优化 (论文作者在实验中得出的 trick)
WGAN 的个人一些使用经验总结，这些经验是基于自身的实验得出，仅供参考：

WGAN 的论文指出使用 MLP，3 层 relu，最后一层使用 linear 也能达到可以接受的效果，但根据我实验的经验上，可能对于彩色图片，因为其数值分布式连续，所以使用 linear 会比较好。但针对于 MINST 上，因为其实二值图片，linear 的效果很差，可以使用 batch normalization sigmoid 效果更好。
不要在 D 中使用 batch normalization，估计原因是因为 weight clip 对 batch normalization 的影响
使用逆卷积来生成图片会比用全连接层效果好，全连接层会有较多的噪点，逆卷积层效果清晰。
关于衡量指标，Wasserstein distance 距离可以很好的衡量 WGAN 的训练进程，但这仅限于同一次，即你的代码从运行到结束这个过程内。
WGAN-GP 论文摘要
生成对抗网络（GAN）将生成问题当作两个对抗网络的博弈：生成网络从给定噪声中产生合成数据，判别网络分辨生成器的的输出和真实数据。GAN可以生成视觉上吸引人的图片，但是网络通常很难训练。前段时间，Arjovsky等研究者对GAN值函数的收敛性进行了深入的分析，并提出了Wasserstein GAN（WGAN），利用Wasserstein距离产生一个比Jensen-Shannon发散值函数有更好的理论上的性质的值函数。但是仍然没能完全解决GAN训练稳定性的问题。

在该论文中，蒙特利尔大学的研究者对WGAN进行改进，提出了一种替代WGAN判别器中权重剪枝的方法，下面是他们所做的工作：

通过小数据集上的实验，概述了判别器中的权重剪枝是如何导致影响稳定性和性能的病态行为的。
提出具有梯度惩罚的WGAN（WGAN with gradient penalty），从而避免同样的问题。
展示该方法相比标准WGAN拥有更快的收敛速度，并能生成更高质量的样本。
展示该方法如何提供稳定的GAN训练：几乎不需要超参数调参，成功训练多种针对图片生成和语言模型的GAN架构
WGAN的critic函数对输入的梯度相比于GAN的更好，因此对生成器的优化更简单。另外，WGAN的值函数是与生成样本的质量相关的，这个性质是GAN所没有的。WGAN的一个问题是如何高效地在critic上应用Lipschitz约束，Arjovsky提出了权重剪枝的方法。但权重剪枝会导致最优化困难。在权重剪枝约束下，大多数神经网络架构只有在学习极其简单地函数时才能达到k地最大梯度范数。因此，通过权重剪枝来实现k-Lipschitz约束将会导致critic偏向更简单的函数。如下图所示，在小型数据集上，权重剪枝不能捕捉到数据分布的高阶矩。



由于在WGAN中使用权重剪枝可能会导致不良结果，研究者考虑在训练目标上使用Lipschitz约束的一种替代方法：一个可微的函数是1-Lipschitz，当且仅当它的梯度具有小于或等于1的范数时。因此，可以直接约束critic函数对其输入的梯度范数。新的critic函数为：



实验结果
研究者们在CIFAR-10数据集上将梯度惩罚的WGAN与权重剪枝的WGAN的训练进行了对比。其中橙色曲线的梯度惩罚WGAN使用了与权重剪枝WGAN相同的优化器（RMSProp）和相同的学习率。绿色曲线是使用了Adam优化器和更高学习率的梯度惩罚WGAN。可以看到，即使使用了同样的优化器，该论文中的方法也能更快的收敛并得到更高的最终分数。使用Adam优化器能进一步提高性能。



为了展示该方法训练过程中的稳定性，研究者在LSUN卧室训练集上训练了多种不同的GAN架构，除了DCGAN外，研究者还选择了另外六种较难训练的架构，如下图所示：



对于每种架构，研究者都使用了四种不同的GAN过程:梯度惩罚的WGAN，权重剪枝的WGAN，DCGAN，以及最小二乘GAN。对于每种方法，都使用了推荐的优化器超参数默认设置：

WGAN with gradient penalty: Adam (α = .0001, β1 = .5, β2 = .9)
WGAN with weight clipping: RMSProp (α = .00005)
DCGAN: Adam (α = .0002, β1 = .5)
LSGAN: RMSProp (α = .0001) [chosen by search over α = .001, .0002, .0001]
上图显示的样本都是经过200k次迭代的结果。目前为止，梯度惩罚的WGAN是唯一一种使用同一种默认超参数，并在每个架构下都成功训练的方法。而所有其他方法，都在一些架构下不稳定。

使用GAN构建语言模型是一项富有挑战的任务，很大程度上是因为生成器中离散的输入输出序列很难进行反向传播。先前的GAN语言模型通常凭借预训练或者与监督最大似然方法联合训练。相比之下，使用该论文的方法，不需采用复杂的通过离散变量反向传播的方法，也不需要最大似然训练或fine-tune结构。该方法在Google Billion Word数据集上训练了一个字符级的GAN语言模型。生成器是一个简单的CNN架构，通过1D卷积将latent vector转换为32个one-hot字符向量的序列。

下图展示了模型的一个例子。目前为止，这是第一个完全使用对抗方法进行训练，而没有使用监督的最大似然损失的生成语言模型。其中有一些拼写上的错误，这可能是由于模型是每个字符独立输出的。



该文提供了一种训练GAN的稳定的算法，能够更好的探索哪种架构能够得到最好的生成模型性能。该方法也打开了使用大规模图像或语言数据集训练以得到更强的模型性能的大门。

Github：https://github.com/caogang/wgan-gp
https://github.com/igul222/improved_wgan_training
