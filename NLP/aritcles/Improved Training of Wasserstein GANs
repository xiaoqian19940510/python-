在Reddit上就点评了这篇文章，NYU的又祭出了这篇，令WGAN在NLP上也能发挥威力。

在WGAN中，他们给出的改进方案是：

判别器最后一层去掉sigmoid

生成器和判别器的loss不取log

每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c

不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行

这里引用自知乎专栏：令人拍案叫绝的WassersteinGAN-知乎专栏

文章写得深入浅出，强烈推荐。

其中第三项就是机器翻译文章中也用到的weightclipping，在本文中，他们发现通过weightclipping来对D实施Lipschitz限制（为了逼近难以直接计算的Wasserstein距离），是导致训练不稳定，以及难以捕捉复杂概率分布的元凶。所以文章提出通过梯度惩罚来对Critic（也就是D，WGAN系列都将D称之为Critic）试试Lipschitz限制。

如图：损失函数有原来的部分+梯度惩罚，现在不需要weightclipping以及基于动量的优化算法都可以使用了，他们在这里就用了Adam。同时可以拿掉BatchNormalization。

如图所示，实验结果很惊人，这种WGAN—GP的结构，训练更加稳定，收敛更快，同时能够生成更高质量的样本，而且可以用于训练不同的GAN架构，甚至是101层的深度残差网络。

同时也能用于NLP中的生成任务，而且是character-level的languagemodel，而MaliGAN的实验是在Sentence-Level上面的。而且前面几篇提到的文章2,3,4在对抗性训练的时候或多或少都用到了MLE，令G更够接触到GroundTruth，但是WGAN-GP是完全不需要MLE的部分。
