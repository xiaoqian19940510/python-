题目：Deep Reinforcement Learning for Dialogue Generation
使用的相关技术：强化学习，深度增强学习，LSTM（简要说明），seq2seq对话模型（简要说明）
SEQ2SEQ模型：输入是一个序列，输出也是一个序列。这种结构最重要的地方在于输入序列和输出序列的长度是可变的。基础的Seq2Seq主要包括Encoder，Decoder，以及连接两者的固定大小的State Vector。
最基础的Seq2Seq模型包含了三个部分，即Encoder、Decoder以及连接两者的中间状态向量，Encoder通过学习输入，将其编码成一个固定大小的状态向量S，继而将S传给Decoder，Decoder再通过对状态向量S的学习来进行输出。
  
图中每一个box代表了一个RNN单元，通常是LSTM或者GRU。其实基础的Seq2Seq是有很多弊端的，首先Encoder将输入编码为固定大小状态向量的过程实际上是一个信息“信息有损压缩”的过程，如果信息量越大，那么这个转化向量的过程对信息的损失就越大，同时，随着sequence length的增加，意味着时间维度上的序列很长，RNN模型也会出现梯度弥散。最后，基础的模型连接Encoder和Decoder模块的组件仅仅是一个固定大小的状态向量，这使得Decoder无法直接去关注到输入信息的更多细节。由于基础Seq2Seq的种种缺陷，随后引入了Attention（添加入了Attention注意力分配机制后，使得Decoder在生成新的Target Sequence时，能得到之前Encoder编码阶段每个字符的隐藏层的信息向量Hidden State，使得生成新序列的准确度提高。）的概念以及Bi-directional encoder layer等。
一、	论述问什么使用强化学习
回报最大化，不会直接指示选择哪种行动，一系列的actions和奖励信号都会对以后有影响，而其他机器学习算法是学习怎么做。选择强化学习的原因：（1）相比监督学习，监督学习的label是其应该执行的正确行为，而对交互的问题处理效果不好，不能从自己的经历中学习，不能对当前的信息评估以优化下一个行为。（2）无监督学习是从一堆样本中发现其隐藏的结构，不适用。所以少RL不需要label，只需要reward信号，反馈是延迟的，不是立即生成的，agent的行为会影响之后的data，刚好适用于对话系统中。
SEQ2SEQ（2014）存在两大缺点会生成通用回复；容易陷入死循环。改进方向：有预见性；开发者定义奖励函数
二、	介绍强化学习（策略函数和价值函数）
不是告诉agent如何产生正确的动作，而是agent对所产生的动作评估其好坏，agent根据自己的学习经历训练出更符合自身特点的网络，通过agent对动作的评估来学习，以适应环境。其目标是动态地调整参数，以达到强化信号最大。所以系统中需要一个随机单元，使agent在可能的动作空间中进行搜索并发现正确的动作，具有动态学习的特性。强化学习中两个重要的概念是：探索（exploration新的东西）和开发（exploitation），不去探索可能找到的是局部最优值。RL问题四个重要概念：（1）规则（policy）：agents在特定时间特定环境的行为方式；（2）奖励信号（a reward signal）：每个时间点agent返回给另一个agent的信号，根据reward来调整policy；（3）值函数（value function）：相比reward是长期收益，可以看作是reward的累加；（4）环境模型（a model of the environment）：agent和environment交互见下图。。。。总结即寻找策略使回报最大。
三、	论述本文如何将深度增强学习应用与对话系统
学习系统是由两个代理组成，使用P表示第一个代理生成的句子，q表示第二个代理生成的句子。两个代理轮流与对方交谈。一个对话可以用两个代理生成的句子交换序列表示，比如：p1; q1; p2; q2; ...; pi; qi。策略梯度方法比Q-学习更适合我们的场景，因为能够使用已经产生反应的MLE参数去初始化编码-译码RNN。而Q-学习直接估计每个动作的未来期望奖励，这会在数量级上不同于MLE结果，会使得MLE参数不适合用于初始化。
（1）动作action
动作a是生成对话的表达，该动作空间是无限的，因为可以生成任意长度的序列。
（2）状态state
一个状态可以用先前两个对话回合[pi;qi]表示，通过把级联的pi和qi注入到LTSM编码器模型中，对话记录可以被进一步的转换为一个向量的形式。
（3）策略policy
策略采取一个LTSM编码-译码器的形式（也就是，PRL(pi+1|pi,qi)）并通过其参数定义。注意到策略是一个随机表示的（在动作给定状态上的概率分布），一个确定性的策略将导致不连续目标，且该目标难于使用基于梯度算法进行优化。
（4）奖励reward
r表示每一个行动获得的奖励。
简化回答：由机器生成的行为应该很容易回应，这部份与其前向函数有关。提出的方法使用负对数似然表示对应的迟钝反应的对话来简化回答。手动构造一个迟钝反应的列表S，比如“我不知道你在干什么”，“我没有主意”等等。这在SEQ2SEQ对话模型中可以经常被发现，奖励函数可以用下式表达：
 
pseq2seq表示似然输出，另外注意到pseq2seq与随机策略函数PRL(pi+1|pi,qi)不同，前者的学习是基于SEQ2SEQ模型的MLE目标，而后者是对在RL集中的长期未来奖励的策略优化。r1是进一步扩大为目标长度S。
四、模拟
算法背后的中心思想去模拟两个虚拟代理互相交流的过程，通过探索状态空间和学习策略PRL(pi+1|pi,qi)使得最优期望的奖励。
（1）监督式学习。第一阶段的训练，建立在之前的预测生成的目标序列工作上。来自监督模型的结果将稍后用于初始化。在Open subtitle数据集上训练SEQ2SEQ模型，该数据集是由8000万原目标对组成。在数据集中，将每个回合看成是一个目标和两个先前句子楚串联做为源输入。
（2）互信息。从简单的 seq2seq 模型中得到的雏形agent通常是非常低效、愚笨的，例如：「我不知道」的频繁出现。所以，即便是作为雏形的agent，简单地采用seq2seq的结果也容易导致多样性极大地缺失。借用之前Li 自己一篇文章，本篇通过对source-to-target的互信息进行建模，明显减少了回答出无意义答案的概率，并且普遍提高了响应的回答质量。具体来说，仅仅采用之前 Eq 3的算法是不太现实的，因为第二项需要等到之前的响应完全被计算出来之后才可以继续计算。于是借用最近的文章的概念，把生成最大互信息的问题可以当做一个强化学习的问题：在模型生成一个序列的结尾的时候，我们可以把当时的互信息值作为一个奖惩项加入到模型中。在这里，采用策略梯度的方法进行优化。首先，用基础的 seq2seq 生成模型作为预训练模型，对于一个给定的输入[pi,qi]，可以根据模型生成一个候选回答集合A。对于A中的每一个回答a,从预训练模型中得到的概率分布上可以计算出互信息的值 m(a,[pi,qi])。这个互信息的值会在此时作为奖励加入到encoder-decoder的模型当中去，这样就可以指引模型朝着生成更高的奖励的方向进行演化。另外，在训练过程中，遵循了随机梯度下降的更新策略，同时由于训练目标是最大似然估计和奖励值之间的组合，所以也借用了Bengio在09年提出的课程学习（Curriculum learning）的策略来作为更新的策略和手段。
（3）两个 agent 之间的对话模拟。模拟两个 agent 的轮流对话的过程是这样的，在一开始，从训练集中随意找到一句话作为输入给第一个agent，这个agent 通过encoder网络把这个输入编码成一个隐层向量，然后decode来生成响应。之后，第二个agent把刚才前一个agent输出的响应和对话历史融合，重新通过encoder网络编码得到一个隐层向量（相当于更新了对话的状态），然后通过decoder网络生成响应，并传给第一个agent。这个过程不断被重复。整个过程如图：
 
更具体地，把之前利用互信息训练过的模型作为初始模型，然后利用策略梯度方法来更新参数，以达到一个比较大的期待奖励值的。对于一系列的响应，其奖励函数为：
 
之后用强化学习的梯度来进行更新。另外，对于每一个模拟的实例，每一次对话响应产生五个候选集合。又由于大多数的响应得分较高的选项其实都是比较类似的，只有表单符号和时态上的一些变化，所以候选集合采用混合了高斯噪声的分布函数来进行采样，在一定程度上丰富了候选集合的多样性。
（4）课程学习（Curriculum Learning）
在最终模型的训练当中，课程学习的策略又一次被使用了。
模型起初只局限于两轮，后来慢慢增加到多轮。因为每一轮的候选集合数目固定，所以每增加一轮，整个路径空间就成倍变大，呈现一个指数级别的增长状态，所以最终模型最多限定为五轮对话。
五、实验结果展示
整个章节用来展示实验结果，并做一些定量定性的分析。评估由人和机器指标共同完成，其中机器指标包括两个方面：对话的长度和多样性。
（1）数据集说明
对于模拟agent来说，最初接受到的输入需要具备一个比较高的质量才比较有利。例如，如果最初的输入是「why?」，则非常难以开展之后的对话，因为实在无法搞清楚对话进行的方向。所以，文章从Opensubtile的一千万条数据中，选出了80万个最不太可能产生「I don’t know what you are talking about」的序列来作为初始输入的集合。
（2）机器自动评价
评价一个对话系统是非常困难的。尽管存在例如BLEU值、困惑度这些传统的指标可以用来评价对话质量，但是在对于这篇文章的框架而言确实不合适的。因为它的训练目标不是为了得到一个跟训练集高度吻合，用以得到最有可能正确的”答案”，而是更符合逻辑的、更成功的、更长的对话，所以并没有采用以上提到的两个指标作为评价。
这里采用的指标主要有：对话的长度 ：这里对一个对话结束的标准是这样定义的，如果一个对话得到了一个例如「I don’t know」这样的无效回答，或者两轮对话之间同一个agent的响应高度一致，则判定这段对话结束。在1000个输入的测试后，平均每个模型能进行的对话长度的数据如图。可以看出，采用了互信息策略的模型，已经可以显著增加对话的长度。在互信息模型的基础上训练的强化学习模型则取得了更好的效果。结果如图：
 
多样性：多样性是用生成响应中的 unigram和bigram的数量来衡量的。 为了避免模型训练出现一个偏向于更长句的倾向，上面的两个数量也要被响应中所有的词汇数量所规约。也就是说出现的一元文法词汇和二元文法词汇（去重的）占所有词汇的比例（重复计数的）。
在标准的 seq2seq 和强化学习模型中，都采用了一个尺度为 10 的 beam search 来生成响应。对于互信息模型，还经过了一个编码—解码—反编码—解码的过程。结果如图：
 
结果表明，强化学习模型比剩余的两种都具备更好的多样性。
（3）人工评价
从模型生成的条目中抽取一定数量的响应，采用众包的方式，对于三个关心的指标进行评价统计。
单轮响应的质量 ：随机选取了 500 个单轮响应作为人工评价的集合，每次将三个模型（基础 seq2seq、带互信息的模型、强化学习的模型）的结果给 3 个人，让他们选择其中哪两个的回答是「更好的」。最后统计每个模型的得分数。
单轮响应的可回答性 ：类似地，随机选取了 500 个单轮响应作为人工评价的集合，每次将三个模型的结果分发给 3 个人。这次让他们选择出哪两个的回答是「更容易进一步被响应的」。
多伦对话的质量 ：第三个指标选取的是模型生成的 200 组多轮对话作为人工评价的集合，每次将三个模型生成的对话分发给 3 个人，让他们选择哪一个对话的质量最高。
人工评价的结果如图所示，其展示地是强化学习得到的比只用了互信息模型的提升程度。
 
可见，在单轮对话的质量上，强化学习并没有明显提升（2%），这与作者们的预期是一致的，因为强化学习设计的优化目标并不是产生单轮的表达，而是更长期的优质对话内容。但是，在单轮对话的「更易被进一步响应」和多轮对话的质量上，强化学习模型都获得了显著的提升。
（4） 定性分析和讨论
随机采样一些实例，可以看到这样的对比结果：
  
可以看出，强化学习产生的响应确实比其他的模型更具有可交互性。同样作者也发现强化学习的模型展现出了一种反问的倾向。更互信息模型想必，强化学习模型生成的对话也更具有交互性，且更加持久。
另外，做一些错误分析也可以得到一些结论。尽管模型在优化中已经加入了对重复话语的惩罚，但是模型产生的响应还是会进入一个循环，但是长度却不止是一轮。如图：
 
这可能归结于在训练过程中的历史对话长度较短引起的。另外一个问题是模型经常会产生一个比较「出戏」的响应，与之前聊的话题不太相关，这是模型在惩罚重复和模型对话相关性之间的一个折衷的反应。
当然，最根本的问题还是在于作者们对于一个好的对话的评价的指标是人为定义的，其还不太能完备地描述好一个理想对话的方方面面。尽管作者已经用一些启发式的奖惩来进行集成一个自动计算的治疗，也确实考虑到了一些构建一个好的对话系统的一些因素，但是终究还是不如从真正的人来接受到的回馈作为反馈（加入到模型训练之中）。另外一个问题是，由于搜索空间和候选集合的指数级别地增长，很难在很大的候选范围内来进行训练，这可能导致模型并没有得到一个最好的训练。
六、结论
作者在这篇文章中介绍了一个强化学习的框架，通过模拟两个agent之间的自动对话来训练一个神经网络对话生成的模型。该模型将基本seq2seq模型和强化学习的算法整合到了一起，利用seq2seq模型得到历史对话的隐层表示，并结合强化学习的算法得到一个在语义上合适的对话响应。这个框架可以更多地考虑一个对话长远的发展来生成响应，从一定意义上可以更好地捕捉一个好的对话的全局特性。除了贡献了这个模型中用到的非常简洁、可操作的对话系统的几个全局特性的数学化指标之外，这个框架也搭建了一个能产生具有更丰富、更多交互性、更能持续响应的对话系统。
七、	搜索是否有相关代码
有相关代码，见github地址：
https://github.com/jiweil/Neural-Dialogue-Generation
使用语言：lua
主要内容：作者主要研究NLP，共享了其代码，内容包括常用模型和强化学习和对抗学习模型。
