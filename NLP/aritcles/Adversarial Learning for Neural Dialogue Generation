题目：Adversarial Learning for Neural Dialogue Generation
中文题目： 基于对抗学习的神经对话生成系统
论文信息：Li J, Monroe W, Shi T, et al. Adversarial Learning for Neural Dialogue Generation[J]. 2017.
研究问题： 为了使对话系统更接近人类的对话，提出基于对抗学习的神经网络对话生成系统，利用图灵测试来判断系统性能。
使用方法： 本文提出的基于对抗学习的神经对话生成系统是基于强化学习的网络，分为两个系统，一个用于生成回复序列的通用模型，一个模拟人类评估的图灵测试鉴频器。
最终效果： 实验结果表明该系统生成的对话模型比之前的基线更接近人类对话。

生成对抗网络（2014）
策略梯度和强化
生成对话网络用于对话生成

1、生成对抗网络介绍


2、策略梯度和强化

3、生成对话网络用于对话生成


用GAN和强化学习来做对话系统，如果我没有记错，这篇paper是最早引用SeqGAN的，有同学还说这篇是最早将RL用到GAN上的，主要是Jiwei大神名气太大，一放上Arxiv就引起无数关注。

如图，文章也是用了PolicyGradientMethod来对GAN进行训练，和SeqGAN的方法并没有很大的区别，主要是用在了DialogueGeneration这样困难的任务上面。还有两点就是：第一点是除了用蒙特卡罗搜索来解决部分生成序列的问题之外，因为MCSearch比较耗费时间，还可以训练一个特殊的D去给部分生成的序列进行打分。但是从实验效果来看，MCSearch的表现要更好一点。

第二点是在训练G的时候同时还用了Teacher-Forcing（MLE）的方法，这点和后面的MaliGAN有异曲同工之处。

为什么要这样做的原因是在对抗性训练的时候，G不会直接接触到真实的目标序列（gold-standardtargetsequence），当G生成了质量很差的序列的时候（生成质量很好的序列其实相当困难），而D又训练得很好，G就会通过得到的Reward知道自己生成的序列很糟糕，但却又不知道怎么令自己生成更好的序列，这样就会导致训练崩溃。所以通过对抗性训练更新G的参数之后，还通过传统的MLE就是用真实的序列来更新G的参数。类似于有一个“老师”来纠正G训练过程中出现的偏差，类似于一个regularizer。
