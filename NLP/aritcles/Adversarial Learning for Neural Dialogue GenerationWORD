题目：Adversarial Learning for Neural Dialogue Generation
一、生成对抗网络介绍
强化学习通过奖励函数的最大化来达到训练的目的，但是很多应用场景很难找到一个奖励函数来精确的度量周边环境对下一个动作影响因子。生成式对抗网络：一种深度学习模型，主要包括两个模块：生成模型（Generative model）和判别模型（Discriminative model）的互相博弈产生相当好的输出。，一般两个模块均使用深度神经网络，
 
生成器(Generator)和鉴别器(Discriminator)都是常见的卷积+全连网络。前者从随机向量生成一个样本，后者鉴别生成的样本以及训练集样本究竟谁真谁假。两者同时训练。在训练鉴别器时，最小化鉴别误差；在训练生成器时，最大化鉴别误差。两个目的均可以通过反向传播方法实现。训练好的生成网络，能把任意一个噪声向量转化成和训练集类似的样本。该噪声可以看做该样本在低维空间的编码。
二、模型
（1）Generative model & Discriminativemodel:
生成器G就是一个seq2seq模型，输入是历史对话x，通过RNN来对语义进行向量表示再逐一生成回答的每个词，从而形成回答y，由于该种模型已经有很大的人群认知，就不过多赘述。判别器D是一个输入为历史对话x和回答y二元组的一个二分类器，使用了hierarchicalencoder，其中机造回答组合为负例Q−({x,y})，人造回答组合为正例Q+({x, y})。
（2）Policy Gradient Training:
文中模型采用了policy gradient的方法（增强学习的方式之一）来进行增强学习的训练，其优化目标为：
 
优化目标的导数可化为：
 
Q是判别器D的结果，换句话说判别器的鉴定结果可以看作是增强学习中的reward，policy gradient整体的优化目标其实就是希望回报高的决策其概率越高。在本任务中state为x，即历史对话；决策为y，即下一步的对话。x生成y的概率等于逐词生成的概率，如（2）所示，这里也可以很好的和seq2seq的工作机理对应上。b({x,y})则是一个使得训练稳定的平衡项。
（3）Reward for EveryGeneration Step (REGS) ：
在本任务中增强学习的一个很大的问题在于我们的估价都是针对一整个回答的，判别器只会给出一个近似于对或者不对的反馈。这样的模式存在一个很大的问题是，即使是很多被判断为有问题的句子，其中有很大一部分语言成分是有效的，如文中的例子“what’s yourname”，人类回答“I am John”，机器回答“I don’t know”。判别器会给出“I don’t know”是有问题的，但无法给出I是对的而后面的don’t know是错的，事实上机器没有回答he/she/you/they而是I本质上是需要一个肯定的正反馈的。判别器只告诉机器对或错，却不告知哪部分对和哪部分错，这对训练带来了很大隐患。所以文中采用了两种方式，第一种是Monte Carlo，第二种则是使用局部序列来评估。第一种和之前的一些增强学习引入的工作类似就不做赘述，我们主要关注第二种方法。
  
主要思想就是将二式变为三式，通俗点讲就是把序列评分拆开来算，这样就能算到前缀的评分，做到局部评价的反馈。为了防止训练过拟合，每次只是从正例和负例的子序列中随机选取一个来训练。不过有一丝遗憾的是，这个方法快速也符合常识但会使得判别器变弱，实际效果不如Monte Carlo准确。
（4）Teacher Forcing ：
在以往的工作中，D效果非常好而G的效果非常糟糕会带来训练效果的下降。试想一下一个G所有产生的答案都被D驳回了，在这段时间内G的所有反馈都是负反馈，G就会迷失从而不知道向什么方向优化会得到正反馈，所以理想的情况下G和D是交替训练上升的。在控制D和G训练节奏的同时，这篇工作中又采用了一种类似强制学习的方式来尝试解决这个问题。每次在正常的增加学习后会让生成器强行生成正确答案并从D得到正向的反馈，从而每次都能有一个正向优化方向的指示。这样的行为类似于学校老师强行灌输知识，也很类似于之前的professor-forcing算法。所以到此整体的模型结构为：
 
训练频率的设定在图中的解释中有提到。无论是GAN还是RL都是出了名的难训，Training Details大家还是看源码和论文仔细体会吧，在此也就不铺开了。
三、实验
实验的结果以及部分case study可以一定程度上体现对抗训练的模型起到了预期的效果。
 
 
 
 
四、总结
虽然在手法上和之前的SeqGAN类似，采用了增强学习的方法来在NLP任务上进行对抗训练，并且提出了一些新的针对于NLP本身特征的方法改进，尽管还存在很大问题需要解决，但也算是不错的尝试。引入对抗训练这样的方式可以解决以往模型，尤其是无监督生成式任务模型的许多问题，但受限于技术细节，现在的模型还处于探索阶段，模型繁冗复杂，训练过程需要掺入大量工程实现手法，方法无法在相似任务上灵活转移。作者在这篇工作的结论里也提到了自己的模型在其他任务诸如machine translation和summarization效果并不是很好，并给出了自己认为可能的一些解释。这些都需要我们进一步的去探索挖掘文本本身特有的性质来改造发展模型，并进一步尝试。

